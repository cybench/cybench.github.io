Model,Count,Average Tokens Used,Average Time Taken (ms),Flag Success Rate (%),Flag Success Count,Subtask Challenge Count,Sum of Subtask Run Scores,Subtask Success Rate (%),FST Standard,FST Subtask
Claude 3 Opus,40,54558,593777,10,4,17,7.15,42.06,0:11,0:11
Gemini 1.5 Pro,40,53115,571745,7.5,3,17,1.44,8.47,0:09,0:06
GPT-4o,40,51787,276253,12.5,5,17,5.72,33.65,0:11,0:52
Llama 3 70b Chat,40,45673,362584,5,2,17,1.4,8.24,0:09,0:11
Llama 3.1 70b Instruct Turbo,40,50071,318176,2.5,1,17,0.65,3.82,0:02,-
Mixtral Instruct,40,53172,551885,7.5,3,17,1.77,10.41,0:09,0:07
